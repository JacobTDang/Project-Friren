#!/usr/bin/env python3
"""
Advanced Complete Demo: Enhanced Orchestrator with News Pipeline Integration

This comprehensive demonstration shows the complete end-to-end process including:
- Real-time news collection simulation with multiple sources
- Advanced FinBERT sentiment analysis with confidence scoring
- XGBoost recommendation engine with feature engineering
- Queue-based communication with decision engine
- Performance analytics and monitoring dashboards
- Error handling and recovery scenarios
- Resource management and optimization
- Complete system lifecycle management
"""

import asyncio
import time
import json
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import threading
from queue import Queue
import statistics
import pandas as pd


class MarketRegime(Enum):
    """Market regime detection"""
    BULLISH = "bullish"
    BEARISH = "bearish"
    SIDEWAYS = "sideways"
    HIGH_VOLATILITY = "high_volatility"


class NewsSource(Enum):
    """News data sources"""
    ALPHA_VANTAGE = "alpha_vantage"
    NEWSAPI = "newsapi"
    REDDIT = "reddit"
    TWITTER = "twitter"
    BENZINGA = "benzinga"
    FINVIZ = "finviz"


class SentimentLabel(Enum):
    """FinBERT sentiment labels"""
    POSITIVE = "POSITIVE"
    NEGATIVE = "NEGATIVE"
    NEUTRAL = "NEUTRAL"


@dataclass
class NewsArticle:
    """Individual news article structure"""
    symbol: str
    title: str
    summary: str
    source: NewsSource
    timestamp: datetime
    url: str
    relevance_score: float
    market_impact: float
    sentiment_raw: Optional[float] = None
    sentiment_confidence: Optional[float] = None
    sentiment_label: Optional[SentimentLabel] = None


@dataclass
class FinBERTResult:
    """FinBERT analysis result"""
    article_id: str
    sentiment_label: SentimentLabel
    confidence: float
    positive_score: float
    negative_score: float
    neutral_score: float
    processing_time: float


@dataclass
class XGBoostRecommendation:
    """XGBoost trading recommendation"""
    symbol: str
    action: str  # BUY, SELL, HOLD
    confidence: float
    prediction_score: float
    feature_importance: Dict[str, float]
    reasoning: str
    risk_score: float
    expected_return: float
    time_horizon: str


@dataclass
class SystemMetrics:
    """System performance metrics"""
    cpu_usage: float
    memory_usage_mb: float
    disk_usage_gb: float
    network_latency_ms: float
    api_calls_remaining: Dict[str, int]
    error_rate: float
    uptime_hours: float


class AdvancedNewsCollector:
    """Advanced news collection with multiple sources and intelligent filtering"""

    def __init__(self):
        self.sources = list(NewsSource)
        self.collection_stats = {source: 0 for source in self.sources}
        self.quality_threshold = 0.7
        self.rate_limits = {
            NewsSource.ALPHA_VANTAGE: {"calls": 500, "remaining": 500},
            NewsSource.NEWSAPI: {"calls": 1000, "remaining": 1000},
            NewsSource.REDDIT: {"calls": 2000, "remaining": 2000},
        }

    async def collect_news_for_symbol(self, symbol: str, max_articles: int = 15) -> List[NewsArticle]:
        """Collect news for a specific symbol from multiple sources"""
        print(f"     Collecting news for {symbol}...")

        articles = []
        for source in self.sources:
            if self.rate_limits.get(source, {}).get("remaining", 0) > 0:
                source_articles = await self._collect_from_source(source, symbol)
                articles.extend(source_articles)

                # Update rate limits
                if source in self.rate_limits:
                    self.rate_limits[source]["remaining"] -= len(source_articles)

        # Quality filtering and ranking
        high_quality_articles = [a for a in articles if a.relevance_score >= self.quality_threshold]
        high_quality_articles.sort(key=lambda x: x.market_impact, reverse=True)

        final_articles = high_quality_articles[:max_articles]
        print(f"     Collected {len(final_articles)} high-quality articles for {symbol}")

        return final_articles

    async def _collect_from_source(self, source: NewsSource, symbol: str) -> List[NewsArticle]:
        """Simulate collecting from a specific news source"""
        await asyncio.sleep(0.1)  # Simulate API call

        # Generate realistic news articles
        article_templates = [
            f"{symbol} reports strong quarterly earnings, beating analyst expectations",
            f"Breaking: {symbol} announces strategic partnership with major tech company",
            f"Analyst upgrades {symbol} target price citing strong fundamentals",
            f"{symbol} faces regulatory scrutiny over recent business practices",
            f"Insider trading activity detected in {symbol} ahead of earnings",
            f"{symbol} launches innovative product line targeting emerging markets",
            f"Market volatility impacts {symbol} stock performance today",
            f"{symbol} CEO discusses company strategy in exclusive interview"
        ]

        num_articles = random.randint(1, 4)
        articles = []

        for i in range(num_articles):
            title = random.choice(article_templates)
            articles.append(NewsArticle(
                symbol=symbol,
                title=title,
                summary=f"Detailed analysis of {title.lower()}...",
                source=source,
                timestamp=datetime.now() - timedelta(minutes=random.randint(1, 60)),
                url=f"https://example.com/{source.value}/{symbol.lower()}/{i}",
                relevance_score=random.uniform(0.5, 1.0),
                market_impact=random.uniform(0.3, 0.9)
            ))

        self.collection_stats[source] += len(articles)
        return articles


class AdvancedFinBERTAnalyzer:
    """Advanced FinBERT sentiment analysis with confidence scoring and batch processing"""

    def __init__(self):
        self.model_loaded = False
        self.batch_size = 4
        self.processing_stats = {
            "total_processed": 0,
            "average_confidence": 0.0,
            "sentiment_distribution": {label: 0 for label in SentimentLabel}
        }

    async def analyze_articles_batch(self, articles: List[NewsArticle]) -> List[FinBERTResult]:
        """Analyze sentiment for a batch of articles"""
        print(f"     Running FinBERT analysis on {len(articles)} articles...")

        # Simulate model loading if not loaded
        if not self.model_loaded:
            print("    ‚è≥ Loading FinBERT model...")
            await asyncio.sleep(1.0)  # Simulate model loading
            self.model_loaded = True
            print("     FinBERT model loaded")

        results = []

        # Process in batches for memory efficiency
        for i in range(0, len(articles), self.batch_size):
            batch = articles[i:i + self.batch_size]
            batch_results = await self._process_batch(batch)
            results.extend(batch_results)

            # Brief pause between batches to simulate processing
            if i + self.batch_size < len(articles):
                await asyncio.sleep(0.2)

        # Update statistics
        self._update_processing_stats(results)

        print(f"     FinBERT analysis complete. Avg confidence: {self.processing_stats['average_confidence']:.3f}")
        return results

    async def _process_batch(self, articles: List[NewsArticle]) -> List[FinBERTResult]:
        """Process a batch of articles through FinBERT"""
        await asyncio.sleep(0.5)  # Simulate processing time

        results = []
        for article in articles:
            # Simulate realistic FinBERT results
            sentiment_scores = self._generate_realistic_sentiment()

            result = FinBERTResult(
                article_id=f"{article.symbol}_{hash(article.title) % 10000}",
                sentiment_label=max(sentiment_scores, key=sentiment_scores.get),
                confidence=max(sentiment_scores.values()),
                positive_score=sentiment_scores[SentimentLabel.POSITIVE],
                negative_score=sentiment_scores[SentimentLabel.NEGATIVE],
                neutral_score=sentiment_scores[SentimentLabel.NEUTRAL],
                processing_time=random.uniform(0.1, 0.3)
            )

            # Update article with sentiment
            article.sentiment_raw = sentiment_scores[result.sentiment_label]
            article.sentiment_confidence = result.confidence
            article.sentiment_label = result.sentiment_label

            results.append(result)

        return results

    def _generate_realistic_sentiment(self) -> Dict[SentimentLabel, float]:
        """Generate realistic sentiment distribution"""
        # Create realistic sentiment scores that sum to ~1
        base_scores = [random.uniform(0.1, 0.8) for _ in range(3)]
        total = sum(base_scores)
        normalized_scores = [score / total for score in base_scores]

        return {
            SentimentLabel.POSITIVE: normalized_scores[0],
            SentimentLabel.NEGATIVE: normalized_scores[1],
            SentimentLabel.NEUTRAL: normalized_scores[2]
        }

    def _update_processing_stats(self, results: List[FinBERTResult]):
        """Update processing statistics"""
        self.processing_stats["total_processed"] += len(results)

        if results:
            confidences = [r.confidence for r in results]
            self.processing_stats["average_confidence"] = statistics.mean(confidences)

            for result in results:
                self.processing_stats["sentiment_distribution"][result.sentiment_label] += 1


class AdvancedXGBoostEngine:
    """Advanced XGBoost recommendation engine with feature engineering"""

    def __init__(self):
        self.model_loaded = False
        self.feature_weights = {
            "news_volume": 0.25,
            "sentiment_score": 0.30,
            "market_impact": 0.20,
            "source_diversity": 0.15,
            "recency_score": 0.10
        }
        self.recommendation_history = {}

    async def generate_recommendations(self, symbol: str, articles: List[NewsArticle],
                                     sentiment_results: List[FinBERTResult]) -> XGBoostRecommendation:
        """Generate trading recommendation based on news and sentiment"""
        print(f"     Generating XGBoost recommendation for {symbol}...")

        # Load model if needed
        if not self.model_loaded:
            print("    ‚è≥ Loading XGBoost model...")
            await asyncio.sleep(0.5)
            self.model_loaded = True
            print("     XGBoost model loaded")

        # Feature engineering
        features = await self._engineer_features(symbol, articles, sentiment_results)

        # Generate prediction
        prediction_score = await self._predict(features)

        # Determine action and confidence
        action, confidence = self._determine_action(prediction_score, features)

        # Calculate risk metrics
        risk_score = self._calculate_risk(features)
        expected_return = self._estimate_return(prediction_score, risk_score)

        recommendation = XGBoostRecommendation(
            symbol=symbol,
            action=action,
            confidence=confidence,
            prediction_score=prediction_score,
            feature_importance=features,
            reasoning=self._generate_reasoning(features, action, prediction_score),
            risk_score=risk_score,
            expected_return=expected_return,
            time_horizon="1-3 days"
        )

        # Store in history
        self.recommendation_history[symbol] = recommendation

        print(f"     {action} recommendation for {symbol} (confidence: {confidence:.3f})")
        return recommendation

    async def _engineer_features(self, symbol: str, articles: List[NewsArticle],
                                sentiment_results: List[FinBERTResult]) -> Dict[str, float]:
        """Engineer features from news and sentiment data"""
        await asyncio.sleep(0.1)  # Simulate feature engineering

        if not articles:
            return {feature: 0.0 for feature in self.feature_weights}

        # News volume feature
        news_volume = min(len(articles) / 20.0, 1.0)  # Normalize to 0-1

        # Sentiment score feature
        sentiment_scores = [a.sentiment_raw for a in articles if a.sentiment_raw]
        avg_sentiment = statistics.mean(sentiment_scores) if sentiment_scores else 0.5

        # Market impact feature
        impact_scores = [a.market_impact for a in articles]
        avg_impact = statistics.mean(impact_scores) if impact_scores else 0.5

        # Source diversity feature
        unique_sources = len(set(a.source for a in articles))
        source_diversity = min(unique_sources / len(NewsSource), 1.0)

        # Recency score feature
        now = datetime.now()
        recency_scores = [1.0 / (1.0 + (now - a.timestamp).total_seconds() / 3600) for a in articles]
        avg_recency = statistics.mean(recency_scores) if recency_scores else 0.5

        return {
            "news_volume": news_volume,
            "sentiment_score": avg_sentiment,
            "market_impact": avg_impact,
            "source_diversity": source_diversity,
            "recency_score": avg_recency
        }

    async def _predict(self, features: Dict[str, float]) -> float:
        """Simulate XGBoost prediction"""
        await asyncio.sleep(0.05)  # Simulate prediction time

        # Weighted combination of features
        score = sum(features[feature] * weight for feature, weight in self.feature_weights.items())

        # Add some realistic noise
        noise = random.uniform(-0.1, 0.1)
        return max(0.0, min(1.0, score + noise))

    def _determine_action(self, prediction_score: float, features: Dict[str, float]) -> Tuple[str, float]:
        """Determine trading action and confidence"""
        sentiment = features.get("sentiment_score", 0.5)

        if prediction_score > 0.7 and sentiment > 0.6:
            return "BUY", min(0.95, prediction_score + 0.1)
        elif prediction_score < 0.3 and sentiment < 0.4:
            return "SELL", min(0.95, (1.0 - prediction_score) + 0.1)
        else:
            return "HOLD", max(0.5, 1.0 - abs(prediction_score - 0.5))

    def _calculate_risk(self, features: Dict[str, float]) -> float:
        """Calculate risk score"""
        volatility_indicators = [
            1.0 - features.get("source_diversity", 0.5),  # Low diversity = higher risk
            abs(features.get("sentiment_score", 0.5) - 0.5) * 2,  # Extreme sentiment = higher risk
        ]
        return statistics.mean(volatility_indicators)

    def _estimate_return(self, prediction_score: float, risk_score: float) -> float:
        """Estimate expected return"""
        base_return = (prediction_score - 0.5) * 0.1  # -5% to +5% base
        risk_adjusted = base_return * (1.0 - risk_score * 0.5)  # Adjust for risk
        return risk_adjusted

    def _generate_reasoning(self, features: Dict[str, float], action: str, score: float) -> str:
        """Generate human-readable reasoning"""
        sentiment = features.get("sentiment_score", 0.5)
        news_vol = features.get("news_volume", 0.0)

        reasoning_parts = []

        if sentiment > 0.6:
            reasoning_parts.append(f"positive sentiment ({sentiment:.2f})")
        elif sentiment < 0.4:
            reasoning_parts.append(f"negative sentiment ({sentiment:.2f})")

        if news_vol > 0.5:
            reasoning_parts.append(f"high news volume ({news_vol:.2f})")

        if features.get("market_impact", 0.5) > 0.7:
            reasoning_parts.append("high market impact articles")

        if not reasoning_parts:
            reasoning_parts.append("neutral market conditions")

        return f"{action} signal based on {', '.join(reasoning_parts)} (score: {score:.3f})"


class EnhancedSystemMonitor:
    """Advanced system monitoring and analytics"""

    def __init__(self):
        self.start_time = datetime.now()
        self.metrics_history = []
        self.alerts = []

    def get_system_metrics(self) -> SystemMetrics:
        """Get current system performance metrics"""
        return SystemMetrics(
            cpu_usage=random.uniform(10, 80),
            memory_usage_mb=random.uniform(150, 300),
            disk_usage_gb=random.uniform(1, 5),
            network_latency_ms=random.uniform(10, 100),
            api_calls_remaining={
                "alpha_vantage": random.randint(400, 500),
                "newsapi": random.randint(800, 1000),
                "reddit": random.randint(1500, 2000)
            },
            error_rate=random.uniform(0, 0.05),
            uptime_hours=(datetime.now() - self.start_time).total_seconds() / 3600
        )

    def check_system_health(self, metrics: SystemMetrics) -> List[str]:
        """Check system health and generate alerts"""
        alerts = []

        if metrics.cpu_usage > 90:
            alerts.append("HIGH CPU USAGE: Consider reducing batch sizes")

        if metrics.memory_usage_mb > 280:
            alerts.append("HIGH MEMORY USAGE: Close to t3.micro limit")

        if metrics.error_rate > 0.1:
            alerts.append("HIGH ERROR RATE: Check API connections")

        for api, remaining in metrics.api_calls_remaining.items():
            if remaining < 50:
                alerts.append(f"LOW API QUOTA: {api} has {remaining} calls remaining")

        return alerts


class ComprehensiveNewsOrchestrator:
    """Comprehensive orchestrator showing the complete news pipeline process"""

    def __init__(self, symbols: List[str]):
        self.symbols = symbols
        self.news_collector = AdvancedNewsCollector()
        self.finbert_analyzer = AdvancedFinBERTAnalyzer()
        self.xgboost_engine = AdvancedXGBoostEngine()
        self.system_monitor = EnhancedSystemMonitor()

        self.cycle_count = 0
        self.total_articles_processed = 0
        self.total_recommendations = 0
        self.decision_engine_queue = Queue()

        # Performance tracking
        self.performance_history = []
        self.recommendation_history = []

    async def run_complete_news_cycle(self) -> Dict[str, Any]:
        """Run a complete 15-minute news collection and analysis cycle"""
        cycle_start = datetime.now()
        self.cycle_count += 1

        print(f"\n Starting News Cycle #{self.cycle_count}")
        print(f"‚è∞ Timestamp: {cycle_start.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)

        cycle_results = {
            "cycle_number": self.cycle_count,
            "timestamp": cycle_start.isoformat(),
            "symbols_processed": [],
            "total_articles": 0,
            "total_recommendations": 0,
            "processing_time": 0,
            "system_metrics": None,
            "alerts": []
        }

        try:
            # Phase 1: News Collection
            print(" PHASE 1: Multi-Source News Collection")
            print("-" * 50)

            all_articles = {}
            for symbol in self.symbols:
                articles = await self.news_collector.collect_news_for_symbol(symbol)
                all_articles[symbol] = articles
                cycle_results["total_articles"] += len(articles)

                print(f"     {symbol}: {len(articles)} articles collected")

            self.total_articles_processed += cycle_results["total_articles"]

            # Phase 2: FinBERT Sentiment Analysis
            print(f"\n PHASE 2: FinBERT Sentiment Analysis")
            print("-" * 50)

            all_sentiment_results = {}
            for symbol, articles in all_articles.items():
                if articles:
                    sentiment_results = await self.finbert_analyzer.analyze_articles_batch(articles)
                    all_sentiment_results[symbol] = sentiment_results

                    # Display sentiment summary
                    sentiment_counts = {}
                    for result in sentiment_results:
                        label = result.sentiment_label
                        sentiment_counts[label] = sentiment_counts.get(label, 0) + 1

                    print(f"     {symbol} sentiment: {dict(sentiment_counts)}")

            # Phase 3: XGBoost Recommendations
            print(f"\n PHASE 3: XGBoost Recommendation Generation")
            print("-" * 50)

            recommendations = {}
            for symbol in self.symbols:
                articles = all_articles.get(symbol, [])
                sentiment_results = all_sentiment_results.get(symbol, [])

                if articles:
                    recommendation = await self.xgboost_engine.generate_recommendations(
                        symbol, articles, sentiment_results
                    )
                    recommendations[symbol] = recommendation
                    cycle_results["total_recommendations"] += 1

                    print(f"     {symbol}: {recommendation.action} "
                          f"(confidence: {recommendation.confidence:.3f}, "
                          f"expected return: {recommendation.expected_return:+.2%})")

            self.total_recommendations += cycle_results["total_recommendations"]

            # Phase 4: Decision Engine Communication
            print(f"\n PHASE 4: Decision Engine Communication")
            print("-" * 50)

            await self._send_to_decision_engine(recommendations, all_articles, all_sentiment_results)

            # Phase 5: System Monitoring
            print(f"\n PHASE 5: System Health Monitoring")
            print("-" * 50)

            metrics = self.system_monitor.get_system_metrics()
            alerts = self.system_monitor.check_system_health(metrics)

            cycle_results["system_metrics"] = asdict(metrics)
            cycle_results["alerts"] = alerts

            self._display_system_metrics(metrics, alerts)

            # Calculate cycle performance
            cycle_end = datetime.now()
            processing_time = (cycle_end - cycle_start).total_seconds()
            cycle_results["processing_time"] = processing_time

            # Store performance data
            performance_data = {
                "cycle": self.cycle_count,
                "processing_time": processing_time,
                "articles_processed": cycle_results["total_articles"],
                "recommendations_generated": cycle_results["total_recommendations"],
                "memory_usage": metrics.memory_usage_mb,
                "cpu_usage": metrics.cpu_usage
            }
            self.performance_history.append(performance_data)

            print(f"\n Cycle #{self.cycle_count} completed in {processing_time:.2f} seconds")

            return cycle_results

        except Exception as e:
            print(f"\n Error in cycle #{self.cycle_count}: {e}")
            cycle_results["error"] = str(e)
            return cycle_results

    async def _send_to_decision_engine(self, recommendations: Dict[str, XGBoostRecommendation],
                                     articles: Dict[str, List[NewsArticle]],
                                     sentiment_results: Dict[str, List[FinBERTResult]]):
        """Send comprehensive data to decision engine queue"""

        for symbol in recommendations:
            recommendation = recommendations[symbol]
            symbol_articles = articles.get(symbol, [])
            symbol_sentiment = sentiment_results.get(symbol, [])

            # Create comprehensive message
            message = {
                "symbol": symbol,
                "timestamp": datetime.now().isoformat(),
                "cycle_number": self.cycle_count,

                "news_data": {
                    "news_volume": len(symbol_articles),
                    "sources_used": list(set(a.source.value for a in symbol_articles)),
                    "average_relevance": statistics.mean([a.relevance_score for a in symbol_articles]) if symbol_articles else 0,
                    "average_market_impact": statistics.mean([a.market_impact for a in symbol_articles]) if symbol_articles else 0,
                    "staleness_minutes": min([(datetime.now() - a.timestamp).total_seconds() / 60 for a in symbol_articles]) if symbol_articles else 0
                },

                "sentiment_data": {
                    "total_analyzed": len(symbol_sentiment),
                    "average_confidence": statistics.mean([s.confidence for s in symbol_sentiment]) if symbol_sentiment else 0,
                    "sentiment_distribution": {
                        "POSITIVE": len([s for s in symbol_sentiment if s.sentiment_label == SentimentLabel.POSITIVE]),
                        "NEGATIVE": len([s for s in symbol_sentiment if s.sentiment_label == SentimentLabel.NEGATIVE]),
                        "NEUTRAL": len([s for s in symbol_sentiment if s.sentiment_label == SentimentLabel.NEUTRAL])
                    },
                    "processing_time": sum([s.processing_time for s in symbol_sentiment])
                },

                "recommendation": {
                    "action": recommendation.action,
                    "confidence": recommendation.confidence,
                    "prediction_score": recommendation.prediction_score,
                    "reasoning": recommendation.reasoning,
                    "risk_score": recommendation.risk_score,
                    "expected_return": recommendation.expected_return,
                    "time_horizon": recommendation.time_horizon,
                    "feature_importance": recommendation.feature_importance
                },

                "pipeline_metadata": {
                    "cycle_number": self.cycle_count,
                    "processing_timestamp": datetime.now().isoformat(),
                    "total_articles_session": self.total_articles_processed,
                    "total_recommendations_session": self.total_recommendations
                }
            }

            # Add to queue
            self.decision_engine_queue.put(message)
            print(f"     Sent comprehensive message for {symbol} to decision engine")

    def _display_system_metrics(self, metrics: SystemMetrics, alerts: List[str]):
        """Display detailed system metrics"""
        print(f"     CPU Usage: {metrics.cpu_usage:.1f}%")
        print(f"     Memory Usage: {metrics.memory_usage_mb:.1f}MB / 1000MB (t3.micro)")
        print(f"     Disk Usage: {metrics.disk_usage_gb:.2f}GB")
        print(f"     Network Latency: {metrics.network_latency_ms:.1f}ms")
        print(f"      Uptime: {metrics.uptime_hours:.2f} hours")
        print(f"     Error Rate: {metrics.error_rate:.3f}")

        print("     API Quota Remaining:")
        for api, remaining in metrics.api_calls_remaining.items():
            print(f"       - {api}: {remaining} calls")

        if alerts:
            print("     System Alerts:")
            for alert in alerts:
                print(f"         {alert}")
        else:
            print("     No system alerts")

    def display_session_summary(self):
        """Display comprehensive session summary"""
        print("\n" + "=" * 80)
        print(" COMPREHENSIVE SESSION SUMMARY")
        print("=" * 80)

        if self.performance_history:
            avg_processing_time = statistics.mean([p["processing_time"] for p in self.performance_history])
            total_articles = sum([p["articles_processed"] for p in self.performance_history])
            total_recommendations = sum([p["recommendations_generated"] for p in self.performance_history])
            avg_memory = statistics.mean([p["memory_usage"] for p in self.performance_history])
            avg_cpu = statistics.mean([p["cpu_usage"] for p in self.performance_history])

            print(f" Total Cycles Completed: {self.cycle_count}")
            print(f" Total Articles Processed: {total_articles}")
            print(f" Total Recommendations Generated: {total_recommendations}")
            print(f"  Average Processing Time: {avg_processing_time:.2f} seconds")
            print(f" Average Memory Usage: {avg_memory:.1f}MB")
            print(f" Average CPU Usage: {avg_cpu:.1f}%")

            # Performance trends
            if len(self.performance_history) > 1:
                latest_time = self.performance_history[-1]["processing_time"]
                first_time = self.performance_history[0]["processing_time"]
                time_trend = ((latest_time - first_time) / first_time) * 100

                print(f" Performance Trend: {time_trend:+.1f}% processing time change")

        # FinBERT statistics
        print(f"\n FinBERT Analysis Statistics:")
        stats = self.finbert_analyzer.processing_stats
        print(f"   Total Processed: {stats['total_processed']}")
        print(f"   Average Confidence: {stats['average_confidence']:.3f}")
        print(f"   Sentiment Distribution: {dict(stats['sentiment_distribution'])}")

        # News collection statistics
        print(f"\n News Collection Statistics:")
        for source, count in self.news_collector.collection_stats.items():
            print(f"   {source.value}: {count} articles")

        # XGBoost recommendations summary
        print(f"\n XGBoost Recommendations Summary:")
        if self.xgboost_engine.recommendation_history:
            actions = [r.action for r in self.xgboost_engine.recommendation_history.values()]
            action_counts = {action: actions.count(action) for action in set(actions)}
            confidences = [r.confidence for r in self.xgboost_engine.recommendation_history.values()]

            print(f"   Action Distribution: {action_counts}")
            print(f"   Average Confidence: {statistics.mean(confidences):.3f}")

        print(f"\n Decision Engine Queue Status:")
        print(f"   Messages in Queue: {self.decision_engine_queue.qsize()}")


async def run_comprehensive_demo():
    """Run the comprehensive news pipeline demonstration"""
    print(" ADVANCED COMPREHENSIVE NEWS PIPELINE DEMONSTRATION")
    print(" 15-Minute News Collection & FinBERT Analysis with XGBoost Recommendations")
    print("=" * 90)

    # Configuration
    symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META']
    print(f" Monitoring Symbols: {symbols}")
    print(f"‚è∞ Simulating 15-minute intervals")
    print(f" Advanced features: Multi-source collection, FinBERT analysis, XGBoost recommendations")

    # Initialize orchestrator
    orchestrator = ComprehensiveNewsOrchestrator(symbols)

    try:
        # Run multiple cycles to show progression
        num_cycles = 3
        print(f"\n Running {num_cycles} complete news cycles...")

        for cycle in range(num_cycles):
            print(f"\n{'' if cycle == 0 else ''} CYCLE {cycle + 1}/{num_cycles}")

            # Run complete cycle
            cycle_results = await orchestrator.run_complete_news_cycle()

            # Brief pause between cycles
            if cycle < num_cycles - 1:
                print(f"\n‚è≥ Waiting 5 seconds before next cycle...")
                await asyncio.sleep(5)

        # Display comprehensive summary
        orchestrator.display_session_summary()

        # Show queue contents
        print(f"\n DECISION ENGINE QUEUE CONTENTS:")
        print("-" * 50)
        queue_messages = []
        while not orchestrator.decision_engine_queue.empty():
            message = orchestrator.decision_engine_queue.get()
            queue_messages.append(message)

        for i, message in enumerate(queue_messages, 1):
            print(f"\nMessage {i}: {message['symbol']} - {message['recommendation']['action']}")
            print(f"   Confidence: {message['recommendation']['confidence']:.3f}")
            print(f"   Expected Return: {message['recommendation']['expected_return']:+.2%}")
            print(f"   News Volume: {message['news_data']['news_volume']} articles")
            print(f"   Sentiment: {message['sentiment_data']['sentiment_distribution']}")

        print(f"\n DEMONSTRATION COMPLETED SUCCESSFULLY!")
        print("=" * 90)

        print(f"\n INTEGRATION READY FOR PRODUCTION:")
        print("   ‚Ä¢ 15-minute news collection cycles ")
        print("   ‚Ä¢ Multi-source news aggregation ")
        print("   ‚Ä¢ Advanced FinBERT sentiment analysis ")
        print("   ‚Ä¢ XGBoost trading recommendations ")
        print("   ‚Ä¢ Queue-based decision engine communication ")
        print("   ‚Ä¢ Comprehensive system monitoring ")
        print("   ‚Ä¢ Resource optimization for t3.micro ")

        print(f"\n NEXT STEPS:")
        print("   1. Deploy to your Friren_V1 directory")
        print("   2. Configure API keys for news sources")
        print("   3. Integrate with your existing orchestrator.py")
        print("   4. Monitor 15-minute cycles in production")
        print("   5. Customize XGBoost features for your strategy")

    except Exception as e:
        print(f"\n Demonstration error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Run the comprehensive demonstration
    asyncio.run(run_comprehensive_demo())
